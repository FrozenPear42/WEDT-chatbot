{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import io\n",
    "import csv\n",
    "import emoji\n",
    "\n",
    "sys.path.insert(0, '../tf_glove')\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from tf_glove import GloVeModel\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    w = '<start> ' + ' '.join(w.split()) + ' <end>'\n",
    "    return w\n",
    "\n",
    "def create_dataset(path, num_examples):\n",
    "    dataset = []\n",
    "    with open(path, newline='',  encoding=\"utf8\") as data_file:\n",
    "        reader = csv.reader(data_file, delimiter=',', quotechar='\\\"')\n",
    "        for row in reader:\n",
    "            side_a = '<start> ' + row[0] + ' <end>'\n",
    "            side_b = '<start> ' + row[1] + ' <end>'\n",
    "            # normalize all whitespaces to space\n",
    "            side_a = ' '.join(side_a.split())\n",
    "            side_b = ' '.join(side_b.split())\n",
    "            dataset.append([side_a, side_b])\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def tokenize(sentences, model):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tensor = []\n",
    "    for sentence in sentences:\n",
    "        tokens = [emoji.demojize(token.lower())\n",
    "                  for token in tokenizer.tokenize(sentence)]\n",
    "        vector = []\n",
    "        if len(tokens) > 600:\n",
    "            \n",
    "            print(sentence, len(tokens))\n",
    "\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                vector.append(model.id_for_word(token))\n",
    "            except:\n",
    "                pass\n",
    "        tensor.append(vector)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tensor, padding='post')\n",
    "\n",
    "    print(tensor.shape)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def load_dataset(glove_model, path, num_examples=None):\n",
    "    dataset = create_dataset(path, num_examples)\n",
    "\n",
    "    input_data = map(lambda x: x[0], dataset)\n",
    "    output_data = map(lambda x: x[1], dataset)\n",
    "\n",
    "    input_tensor = tokenize(input_data, glove_model)\n",
    "    target_tensor = tokenize(output_data, glove_model)\n",
    "\n",
    "    return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "71\n71\n71\n71\n72\n72\n72\n72\n72\n72\n72\n(64900, 70)\n78\n92\n149\n79\n92\n316\n77\n101\n86\n87\n81\n120\n80\n126\n79\n91\n115\n96\n129\n171\n95\n217\n136\n73\n96\n360\n79\n79\n303\n93\n123\n71\n105\n82\n860\n135\n502\n290\n87\n185\n91\n108\n87\n93\n154\n185\n87\n146\n142\n97\n154\n136\n287\n127\n84\n101\n244\n90\n320\n86\n76\n72\n168\n146\n84\n111\n89\n78\n71\n106\n116\n109\n88\n89\n238\n207\n86\n184\n164\n657\n71\n182\n129\n1185\n87\n146\n75\n1371\n78\n106\n174\n100\n106\n100\n91\n125\n79\n93\n91\n82\n133\n77\n158\n145\n162\n95\n77\n180\n85\n166\n96\n95\n78\n77\n123\n405\n92\n133\n206\n121\n143\n169\n230\n81\n93\n99\n241\n115\n99\n225\n113\n91\n88\n81\n85\n79\n97\n84\n112\n82\n111\n90\n105\n97\n162\n176\n93\n73\n104\n155\n79\n89\n271\n100\n108\n75\n146\n270\n187\n156\n80\n81\n102\n114\n73\n114\n212\n114\n84\n87\n407\n96\n96\n689\n81\n167\n207\n149\n86\n75\n784\n73\n89\n115\n76\n87\n104\n174\n149\n71\n168\n224\n79\n87\n169\n84\n95\n139\n96\n196\n96\n107\n138\n86\n84\n312\n111\n188\n95\n99\n115\n125\n110\n104\n111\n79\n122\n71\n81\n81\n327\n71\n93\n196\n77\n256\n912\n74\n76\n292\n83\n96\n84\n79\n77\n230\n327\n87\n95\n103\n137\n72\n114\n77\n473\n153\n209\n81\n85\n102\n78\n139\n102\n253\n99\n103\n236\n72\n80\n173\n285\n87\n196\n410\n89\n96\n73\n132\n98\n197\n111\n121\n149\n108\n72\n149\n178\n71\n72\n92\n189\n769\n732\n81\n103\n90\n80\n174\n94\n98\n74\n116\n71\n96\n329\n76\n157\n80\n135\n133\n71\n85\n119\n139\n125\n96\n162\n73\n159\n97\n91\n119\n161\n84\n120\n948\n256\n97\n88\n87\n73\n80\n82\n112\n95\n74\n127\n93\n194\n121\n267\n72\n88\n150\n236\n72\n188\n76\n78\n86\n77\n107\n124\n197\n183\n213\n105\n214\n75\n81\n81\n93\n82\n138\n77\n150\n530\n86\n132\n79\n156\n84\n77\n75\n109\n91\n263\n134\n102\n144\n128\n207\n137\n402\n94\n753\n76\n83\n96\n102\n128\n79\n101\n85\n119\n87\n164\n74\n73\n81\n112\n343\n293\n73\n77\n320\n161\n220\n83\n117\n76\n103\n105\n323\n108\n128\n143\n136\n75\n90\n93\n82\n235\n106\n195\n280\n75\n92\n117\n116\n220\n416\n116\n100\n96\n84\n143\n80\n161\n185\n88\n77\n239\n96\n442\n125\n83\n97\n72\n93\n110\n128\n315\n140\n90\n72\n140\n137\n101\n815\n386\n125\n103\n122\n115\n74\n132\n71\n77\n102\n162\n96\n110\n232\n81\n87\n101\n216\n86\n117\n174\n349\n74\n113\n210\n361\n122\n77\n81\n127\n101\n81\n118\n113\n85\n108\n78\n127\n86\n114\n162\n103\n75\n106\n170\n107\n94\n103\n85\n119\n77\n130\n166\n85\n97\n102\n71\n134\n119\n89\n93\n126\n219\n217\n135\n127\n121\n102\n133\n78\n117\n85\n98\n153\n74\n122\n213\n130\n161\n114\n(64900, 1301)\n"
    }
   ],
   "source": [
    "glove_model = GloVeModel()\n",
    "glove_model.load_from_file('../data/glove.local.txt')\n",
    "\n",
    "xd = load_dataset(glove_model, '../data/reddit_merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38064bit604095896627458f9181f3803955bc61",
   "display_name": "Python 3.8.0 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}