{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\Wojciech\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "with open('../data/reddit_merged.csv', newline='',  encoding=\"utf8\") as data_file:\n",
    "    reader = csv.reader(data_file, delimiter=',', quotechar='\\\"')\n",
    "    for row in reader:\n",
    "        side_a = row[0]\n",
    "        side_b = row[1]\n",
    "        # normalize all whitespaces to space\n",
    "        side_a = ' '.join(side_a.split())\n",
    "        side_b = ' '.join(side_b.split())\n",
    "        dataset.append([side_a, side_b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "messages = []\n",
    "for row in dataset:\n",
    "    if row[0] not in messages:\n",
    "        messages.append(row[0])\n",
    "    if row[1] not in messages:\n",
    "        messages.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenized_sentences = []\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "for message in messages:\n",
    "    message =  ' '.join([sentence.lower() for sentence in sent_tokenize(message)])\n",
    "    message = '<start> ' + message + ' <end>'\n",
    "    sentences = [emoji.demojize(token) for token in tokenizer.tokenize(message)]\n",
    "    tokenized_sentences.append([sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for sentences in tokenized_sentences:\n",
    "    corpus += sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['<start>',\n \"couldn't\",\n 'find',\n 'all',\n 'the',\n 'ones',\n 'i',\n 'wanted',\n ',',\n 'but',\n 'u',\n 'was',\n 'trying',\n 'to',\n 'get',\n 'moana',\n '<end>']"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "corpus[2001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "62914"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From D:\\LIBS\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1659: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nWARNING:tensorflow:From ../tf_glove\\tf_glove.py:132: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nDeprecated in favor of operator or tf.math.divide.\nWARNING:tensorflow:From D:\\LIBS\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\training\\adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\n"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../tf_glove')\n",
    "from tf_glove import GloVeModel\n",
    "\n",
    "glove_model = GloVeModel(embedding_size=100, context_size=10, min_occurrences=1, learning_rate=0.05, batch_size=512)\n",
    "glove_model.fit_to_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Writing TensorBoard summaries to ../glove_logs\nwords: 27432\n"
    }
   ],
   "source": [
    "glove_model.train(num_epochs=500, log_dir=\"../glove_logs\", summary_batch_interval=5000)\n",
    "\n",
    "print('words:', len(glove_model.words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model.save_to_file('../data/glove.local.all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "glove_model.generate_tsne(path=\"../map_full.png\", word_count=len(glove_model.words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python38064bit604095896627458f9181f3803955bc61"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}