{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\Wojciech\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "dataset = []\n",
    "with open('../data/reddit.csv', newline='',  encoding=\"utf8\") as data_file:\n",
    "    reader = csv.reader(data_file, delimiter=',', quotechar='\\\"')\n",
    "    for row in reader:\n",
    "        id = row[0]\n",
    "        # replace emojis with tokens\n",
    "        side_a_1 = row[1]\n",
    "        side_b_1 = row[2]\n",
    "        side_a_2 = row[3]\n",
    "        # normalize all whitespaces to space\n",
    "        side_a_1 = ' '.join(side_a_1.split())\n",
    "        side_b_1 = ' '.join(side_b_1.split())\n",
    "        side_a_2 = ' '.join(side_a_2.split())\n",
    "\n",
    "        dataset.append([id, side_a_1, side_b_1, side_a_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "messages = []\n",
    "for row in dataset:\n",
    "    if row[1] not in messages:\n",
    "        messages.append(row[1])\n",
    "    if row[2] not in messages:\n",
    "        messages.append(row[2])\n",
    "    if row[3] not in messages:\n",
    "        messages.append(row[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenized_sentences = []\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "for message in messages:\n",
    "    sentences = [ [emoji.demojize(token.lower()) for token in tokenizer.tokenize(sentence)] for sentence in sent_tokenize(message)]\n",
    "    tokenized_sentences.append(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for sentences in tokenized_sentences:\n",
    "    corpus += sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['my',\n 'mistake',\n 'sorry',\n 'about',\n 'that',\n 'no',\n 'i',\n \"didn't\",\n 'forget',\n 'about',\n 'you',\n 'at',\n 'all',\n ':grinning_face:',\n ':smiling_face_with_smiling_eyes:',\n ':smiling_face:']"
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "corpus[94]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../tf_glove')\n",
    "from tf_glove import GloVeModel\n",
    "\n",
    "glove_model = GloVeModel(embedding_size=100, context_size=5, min_occurrences=10, learning_rate=0.05, batch_size=512)\n",
    "glove_model.fit_to_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Writing TensorBoard summaries to ../glove_logs\nwords: 4052\n"
    }
   ],
   "source": [
    "glove_model.train(num_epochs=300, log_dir=\"../glove_logs\", summary_batch_interval=5000)\n",
    "\n",
    "print('words:', len(glove_model.words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "glove_model.generate_tsne(path=\"../map.png\", word_count=6849)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(text: str):\n",
    "   tokenizer = TweetTokenizer()\n",
    "   tokens = [emoji.demojize(token.lower()) for token in tokenizer.tokenize(text)]\n",
    "   vectors = [glove_model.embedding_for(token) for token in tokens]\n",
    "   return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = prepare_input(\"I <3 your birthday party!\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python38064bit604095896627458f9181f3803955bc61"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}